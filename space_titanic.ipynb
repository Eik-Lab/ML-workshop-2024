{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "print(\"Full train dataset shape is {}\".format(data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = data.Transported.value_counts()\n",
    "plot_df.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5,1,  figsize=(10, 10))\n",
    "plt.subplots_adjust(top = 2)\n",
    "\n",
    "sns.histplot(data['Age'], color='b', bins=50, ax=ax[0]);\n",
    "sns.histplot(data['FoodCourt'], color='b', bins=50, ax=ax[1]);\n",
    "sns.histplot(data['ShoppingMall'], color='b', bins=50, ax=ax[2]);\n",
    "sns.histplot(data['Spa'], color='b', bins=50, ax=ax[3]);\n",
    "sns.histplot(data['VRDeck'], color='b', bins=50, ax=ax[4]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separates the target from the data \n",
    "y = data.Transported\n",
    "X=data.iloc[:,0:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PassengerID and Name are not necessary for training\n",
    "X = X.drop(['PassengerId', 'Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split cabin into three separates variables \n",
    "X[[\"Deck\", \"Cabin_num\", \"Side\"]] = X[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "X = X.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the categorical data to numerical values \n",
    "\n",
    "class_labels = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Cabin_num', 'Side']\n",
    "\n",
    "for cl in class_labels:\n",
    "    class_le = LabelEncoder()\n",
    "    Y_le = class_le.fit_transform(X[cl].values)\n",
    "    X[cl] = Y_le\n",
    "\n",
    "y = LabelEncoder().fit_transform(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "column_order = X.columns\n",
    "\n",
    "df_categorical = X[['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side']]\n",
    "df_non_categorical = X.drop(['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side'], axis=1)\n",
    "\n",
    "\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imr = imr.fit(df_categorical.values)\n",
    "imputed_data = imr.transform(df_categorical.values)\n",
    "df_categorical = pd.DataFrame(imputed_data, columns=df_categorical.columns)\n",
    "\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(df_non_categorical.values)\n",
    "imputed_data = imr.transform(df_non_categorical.values)\n",
    "df_non_categorical = pd.DataFrame(imputed_data, columns=df_non_categorical.columns)\n",
    "\n",
    "combined = pd.concat([df_categorical, df_non_categorical], axis=1)\n",
    "X = combined[column_order]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values\n",
    "print('Column number of missing values')\n",
    "for c in X.columns:\n",
    "    n_NaN = X[c].isnull().sum()\n",
    "    print(f'{c:32} {n_NaN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There NaN values in several of the categories which needs to be removed\n",
    "\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(X.values)\n",
    "imputed_data = imr.transform(X.values)\n",
    "\n",
    "X = pd.DataFrame(imputed_data, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the entire dataset \n",
    "X_scaled = (X - np.mean(X, axis=0)) / np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.boxplot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "X_train_scale = (X_train - np.mean(X_train, axis=0)) / np.std(X_train)\n",
    "X_test_scale = (X_test - np.mean(X_train, axis=0)) / np.std(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scale.boxplot()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of classifiers \n",
    "* Perceptron\n",
    "* Adaline \n",
    "* Logistic Regression\n",
    "* Support vector machine\n",
    "* K-nearest Neighbour \n",
    "* Decision trees\n",
    "* Random forest \n",
    "* Naive Bayes \n",
    "* Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_svc = make_pipeline(PCA(), SVC(C=100.0, gamma=0.01, random_state=1))\n",
    "pipe_svc.fit(X_train_scale, y_train)\n",
    "prediction = pipe_svc.predict(X_test_scale)\n",
    "print(accuracy_score(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the classifier\n",
    "classifier = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "classifier.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set and calculate the accuracy\n",
    "y_pred = classifier.predict(X_test_scale)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "submission_id = test_data.PassengerId\n",
    "\n",
    "# Remove Name and passengerID\n",
    "test_data = test_data.drop(['PassengerId', 'Name'], axis=1)\n",
    "\n",
    "# Split cabin into three variables \n",
    "test_data[[\"Deck\", \"Cabin_num\", \"Side\"]] = test_data[\"Cabin\"].str.split(\"/\", expand=True)\n",
    "test_data = test_data.drop('Cabin', axis=1)\n",
    "\n",
    "# Change the categorical values to numerical values\n",
    "class_labels = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Cabin_num', 'Side']\n",
    "\n",
    "for cl in class_labels:\n",
    "    class_le = LabelEncoder()\n",
    "    Y_le = class_le.fit_transform(test_data[cl].values)\n",
    "    test_data[cl] = Y_le\n",
    "\n",
    "# replace Nan values\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imr = imr.fit(test_data.values)\n",
    "imputed_data = imr.transform(test_data.values)\n",
    "\n",
    "test_data = pd.DataFrame(imputed_data, columns=test_data.columns)\n",
    "\n",
    "# Scale the test data\n",
    "test_data_scaled = (test_data - np.mean(X, axis=0)) / np.std(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the test dataset\n",
    "submission = make_pipeline(PCA(), SVC(C=100.0, gamma=0.01, random_state=1))\n",
    "submission.fit(X_scaled, y)\n",
    "predictions = submission.predict(test_data_scaled)\n",
    "bool_predictions = (predictions > 0.5).astype(bool)\n",
    "\n",
    "output = pd.DataFrame({'PassengerId': submission_id,\n",
    "                       'Transported': bool_predictions})\n",
    "\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv for Kaggle \n",
    "sample_submission_df = pd.read_csv('data/sample_submission.csv')\n",
    "sample_submission_df['Transported'] = bool_predictions\n",
    "sample_submission_df.to_csv('data/submission.csv', index=False)\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
